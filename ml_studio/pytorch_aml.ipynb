{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure ML jobs demo: PyTorch - Wine Quality Prediction Model Training\n",
        "\n",
        "In this notebook learn how to use Azure Machine Learning job submission interface to train a PyTorch neural network model. \n",
        "\n",
        "In this example, we will use the wine quality dataset as demonstrated in the PyTorch demo (if you haven't completed it, no worries). We will repeat the pipeline created in the previous, except in this version we'll submit the job to complete in the background. \n",
        "\n",
        "The goal is to predict wine quality (score 0-10) based on physicochemical properties like acidity, sugar content, alcohol level, etc.\n",
        "\n",
        "## Learning Objectives:\n",
        "- Setup the Azure ML client connection\n",
        "- Define and save a conda .yaml file\n",
        "- Create a training script\n",
        "- Configure and submit a job\n",
        "- Monitor and view the jobs output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "You need a compute instance to run the code as it relies upon a custom environment that is not available with \"Serverless Spark Compute\". If you don't have a compute instance, select **Create compute** on the toolbar to first create one.  You can use all the default settings. \n",
        "\n",
        "## Set your kernel\n",
        "\n",
        "* If your compute instance is stopped, start it now.  \n",
        "        \n",
        "* Once your compute instance is running, make sure the that the kernel, found on the top right, is `Python 3.10 - AzureML`.  If not, use the dropdown to select this kernel."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Background -- Command Jobs in Azure Machine Learning\n",
        "\n",
        "To train a model, you need to submit a *job*. The type of job you'll submit in this tutorial is a *command job*. Azure Machine Learning offers several different types of jobs to train models. Users can select their method of training based on complexity of the model, data size, and training speed requirements.  In this tutorial, you'll learn how to submit a *command job* to run a *training script*. \n",
        "\n",
        "A command job is a function that allows you to submit a custom training script to train your model. This can also be defined as a custom training job. A command job in Azure Machine Learning is a type of job that runs a script or command in a specified environment. You can use command jobs to train models, process data, or any other custom code you want to execute in the cloud. \n",
        "\n",
        "In this tutorial, we'll focus on using a command job to create a custom training job that we'll use to train a model. For any custom training job, the below items are required:\n",
        "\n",
        "* compute resource (usually a compute cluster)\n",
        "* environment\n",
        "* data\n",
        "* command job \n",
        "* training script\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Create handle to workspace\n",
        "\n",
        "Before we dive in the code, you need a way to reference your workspace. You'll create `ml_client` for a handle to the workspace.  You'll then use `ml_client` to manage resources and jobs.\n",
        "\n",
        "In the next cell, enter your Subscription ID, Resource Group name and Workspace name. To find these values:\n",
        "\n",
        "1. In the upper right Azure Machine Learning studio toolbar, select your workspace name.\n",
        "2. Copy the value for workspace, resource group and subscription ID into the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677262283435
        },
        "name": "credential"
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# authenticate\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "subscription_id = \"SUBSCRIPTION ID HERE\"\n",
        "resource_group = \"RESOURCE NAME HERE\"\n",
        "workspace_name = \"WORKSPACE NAME HERE\"\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=subscription_id,\n",
        "    resource_group_name=resource_group,\n",
        "    workspace_name=workspace_name,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### NOTE!\n",
        "\n",
        "Creating MLClient will not connect to the workspace. The client initialisation is lazy, it will wait for the first time it needs to make a call (this will happen in the next code cell)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify that the handle works correctly.\n",
        "# If you ge an error here, modify your SUBSCRIPTION, RESOURCE_GROUP, and WS_NAME in the previous cell.\n",
        "ws = ml_client.workspaces.get(WS_NAME)\n",
        "print(ws.location, \":\", ws.resource_group)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a job environment\n",
        "\n",
        "To run your Azure Machine Learning job on your compute resource, you need an environment. An environment lists the software runtime and libraries that you want installed on the compute where youâ€™ll be training. It's similar to your python environment on your local machine or the kernel running in this notebook currently.\n",
        "\n",
        "Azure Machine Learning provides many curated or ready-made environments, which are useful for common training and inference scenarios. \n",
        "\n",
        "In this example, you'll create a custom conda environment for your jobs, using a conda yaml file.\n",
        "\n",
        "First, create a directory to store the file in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677262301389
        },
        "name": "dependencies_dir"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dependencies_dir = \"./dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The cell below uses IPython magic to write the conda file into the directory you just created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "write_model"
      },
      "outputs": [],
      "source": [
        "%%writefile {dependencies_dir}/conda.yaml\n",
        "name: pytorch-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "  - pytorch\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy=1.21.2\n",
        "  - pip=21.2.4\n",
        "  - scikit-learn=1.0.2\n",
        "  - scipy=1.7.1\n",
        "  - pandas>=1.1,<1.2\n",
        "  - pytorch>=1.11.0\n",
        "  - torchvision\n",
        "  - pip:\n",
        "    - inference-schema[numpy-support]==1.3.0\n",
        "    - mlflow==2.8.0\n",
        "    - mlflow-skinny==2.8.0\n",
        "    - azureml-mlflow==1.51.0\n",
        "    - psutil>=5.8,<5.9\n",
        "    - tqdm>=4.59,<4.60\n",
        "    - ipykernel~=6.0\n",
        "    - matplotlib\n",
        "    - seaborn\n",
        "    - azureml-fsspec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The specification contains some usual packages, that you'll use in your job (numpy, pip).\n",
        "\n",
        "Reference this *yaml* file to create and register this custom environment in your workspace:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we package all the componments of the envrionemtn together using the [Environment Class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.environment?view=azure-python). We give it a name, a description, tags, as well as the path to the .yaml file and an operating system image to build upon. In this case we're using a Ubuntu Linux image. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677262314695
        },
        "name": "custom_env_name"
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name = \"aml-pytorch\"\n",
        "\n",
        "custom_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for PyTorch Wine Quality job\",\n",
        "    tags={\"pytorch\": \"1.11.0\"},\n",
        "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        ")\n",
        "custom_job_env = ml_client.environments.create_or_update(custom_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {custom_job_env.name} is registered to workspace, the environment version is {custom_job_env.version}\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure a training job using the command function\n",
        "\n",
        "You create an Azure Machine Learning *command job* to train a model for credit default prediction. The command job runs a *training script* in a specified environment on a specified compute resource.  You've already created the environment and the compute cluster.  Next you'll create the training script. In our specific case, we're training our dataset to produce a classifier using the `GradientBoostingClassifier` model. \n",
        "\n",
        "The *training script* handles the data preparation, training and registering of the trained model. The method `train_test_split` handles splitting the dataset into test and training data. In this tutorial, you'll create a Python training script. \n",
        "\n",
        "Command jobs can be run from CLI, Python SDK, or studio interface. In this tutorial, you'll use the Azure Machine Learning Python SDK v2 to create and run the command job.\n",
        "\n",
        "## Create training script\n",
        "\n",
        "Let's start by creating the training script - the *main.py* python file.\n",
        "\n",
        "First create a source folder for the script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677262322022
        },
        "name": "train_src_dir"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "train_src_dir = \"./src\"\n",
        "os.makedirs(train_src_dir, exist_ok=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script handles the preprocessing of the data, splitting it into test and train data. It then consumes this data to train a tree based model and return the output model. \n",
        "\n",
        "[MLFlow](https://learn.microsoft.com/articles/machine-learning/concept-mlflow) is used to log the parameters and metrics during our job. The MLFlow package facilitates the running and logging of metrics and results for each model Azure trains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "write_main"
      },
      "outputs": [],
      "source": [
        "%%writefile {train_src_dir}/main.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "class WineQualityNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(WineQualityNet, self).__init__()\n",
        "        \n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 16)\n",
        "        self.fc4 = nn.Linear(16, 1)\n",
        "        \n",
        "        # Activation and dropout\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.2)\n",
        "    parser.add_argument(\"--learning_rate\", required=False, default=0.001, type=float)\n",
        "    parser.add_argument(\"--batch_size\", required=False, default=32, type=int)\n",
        "    parser.add_argument(\"--epochs\", required=False, default=100, type=int)\n",
        "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "    args = parser.parse_args()\n",
        "   \n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    # enable autologging\n",
        "    mlflow.pytorch.autolog()\n",
        "\n",
        "    ###################\n",
        "    #<prepare the data>\n",
        "    ###################\n",
        "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
        "\n",
        "    print(\"input data:\", args.data)\n",
        "    print(' ')\n",
        "\n",
        "    # Load wine quality dataset\n",
        "    wine_data = pd.read_csv(args.data)\n",
        "\n",
        "    print(wine_data)\n",
        "    print(' ')\n",
        "    print('Columns:')\n",
        "    print(wine_data.columns.tolist())\n",
        "    print(' ')\n",
        "    \n",
        "    mlflow.log_metric(\"num_samples\", wine_data.shape[0])\n",
        "    mlflow.log_metric(\"num_features\", wine_data.shape[1] - 1)\n",
        "\n",
        "    # Separate features and target\n",
        "    X = wine_data.drop('quality', axis=1).values\n",
        "    y = wine_data['quality'].values\n",
        "\n",
        "    # Split the data into train and test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_train_ratio, random_state=42)\n",
        "    \n",
        "    # Further split training data into train and validation\n",
        "    X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_final)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "    y_train_tensor = torch.FloatTensor(y_train_final)\n",
        "    X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
        "    y_val_tensor = torch.FloatTensor(y_val)\n",
        "    X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "    y_test_tensor = torch.FloatTensor(y_test)\n",
        "\n",
        "    # Create PyTorch datasets and dataloaders\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size)\n",
        "\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "    print(f\"Test samples: {len(test_dataset)}\")\n",
        "    print(f\"Number of features: {X_train_final.shape[1]}\")\n",
        "    ####################\n",
        "    #</prepare the data>\n",
        "    ####################\n",
        "\n",
        "    ##################\n",
        "    #<train the model>\n",
        "    ##################\n",
        "    # Initialize model\n",
        "    model = WineQualityNet(X_train_final.shape[1])\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "    # Lists to store losses for plotting\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(args.epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X).squeeze()\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "        \n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                outputs = model(batch_X).squeeze()\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                total_val_loss += loss.item()\n",
        "        \n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{args.epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluation on test set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_predictions = []\n",
        "        test_targets = []\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            outputs = model(batch_X).squeeze()\n",
        "            test_predictions.extend(outputs.numpy())\n",
        "            test_targets.extend(batch_y.numpy())\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    predictions = np.array(test_predictions)\n",
        "    y_test_array = np.array(test_targets)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_test_array, predictions)\n",
        "    r2 = r2_score(y_test_array, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    print(f\"Test Set Performance:\")\n",
        "    print(f'Test MSE: {mse:.4f}')\n",
        "    print(f'Test RMSE: {rmse:.4f}')\n",
        "    print(f'Test RÂ² Score: {r2:.4f}')\n",
        "\n",
        "    # Visualization of predictions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test_array, predictions, alpha=0.5)\n",
        "    plt.plot([y_test_array.min(), y_test_array.max()], [y_test_array.min(), y_test_array.max()], 'r--', lw=2)\n",
        "    plt.xlabel('Actual Quality')\n",
        "    plt.ylabel('Predicted Quality')\n",
        "    plt.title('Actual vs Predicted Wine Quality')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('predictions_scatter.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Distribution of prediction errors\n",
        "    errors = predictions - y_test_array\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(errors, bins=30, edgecolor='black')\n",
        "    plt.xlabel('Prediction Error')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Prediction Errors')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('error_distribution.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"test_mse\", mse)\n",
        "    mlflow.log_metric(\"test_rmse\", rmse)\n",
        "    mlflow.log_metric(\"test_r2\", r2)\n",
        "    mlflow.log_param(\"learning_rate\", args.learning_rate)\n",
        "    mlflow.log_param(\"batch_size\", args.batch_size)\n",
        "    mlflow.log_param(\"epochs\", args.epochs)\n",
        "    \n",
        "    # Log the plots as artifacts\n",
        "    mlflow.log_artifact('training_history.png')\n",
        "    mlflow.log_artifact('predictions_scatter.png')\n",
        "    mlflow.log_artifact('error_distribution.png')\n",
        "    ###################\n",
        "    #</train the model>\n",
        "    ###################\n",
        "\n",
        "    ##########################\n",
        "    #<save and register model>\n",
        "    ##########################\n",
        "    # Registering the model to the workspace\n",
        "    print(\"Registering the model via MLFlow\")\n",
        "    mlflow.pytorch.log_model(\n",
        "        pytorch_model=model,\n",
        "        registered_model_name=args.registered_model_name,\n",
        "        artifact_path=args.registered_model_name,\n",
        "    )\n",
        "\n",
        "    # Saving the model to a file\n",
        "    mlflow.pytorch.save_model(\n",
        "        pytorch_model=model,\n",
        "        path=os.path.join(args.registered_model_name, \"trained_model\"),\n",
        "    )\n",
        "    ###########################\n",
        "    #</save and register model>\n",
        "    ###########################\n",
        "    \n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this script, once the model is trained, the model file is saved and registered to the workspace. Registering your model allows you to store and version your models in the Azure cloud, in your workspace. Once you register a model, you can find all other registered model in one place in the Azure Studio called the model registry. The model registry helps you organize and keep track of your trained models. \n",
        "\n",
        "## Configure the command\n",
        "\n",
        "Now that you have a script that can perform the classification task, use the general purpose **command** that can run command line actions. This command line action can be directly calling system commands or by running a script. \n",
        "\n",
        "Here, create input variables to specify the input data, split ratio, learning rate and registered model name.  The command script will:\n",
        "* Use the environment created earlier - you can use the `@latest` notation to indicate the latest version of the environment when the command is run.\n",
        "* Configure the command line action itself - `python main.py` in this case. The inputs/outputs are accessible in the command via the `${{ ... }}` notation.\n",
        "* Since a compute resource was not specified, the script will be run on a [serverless compute cluster](https://learn.microsoft.com/azure/machine-learning/how-to-use-serverless-compute?view=azureml-api-2&tabs=python) that is automatically created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677262332367
        },
        "name": "registered_model_name"
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
        "\n",
        "registered_model_name = \"wine_quality_model\"\n",
        "data_asset = ml_client.data.get(\"wine_quality_kaggle\", version=\"1\")\n",
        "\n",
        "job = command(\n",
        "    inputs=dict(\n",
        "        data=Input(\n",
        "            type=AssetTypes.URI_FILE,\n",
        "            path=data_asset.path,\n",
        "        ),\n",
        "        test_train_ratio=0.2,\n",
        "        learning_rate=0.001,\n",
        "        batch_size=32,\n",
        "        epochs=100,\n",
        "        registered_model_name=registered_model_name,\n",
        "    ),\n",
        "    code=\"./src/\",  # location of source code\n",
        "    command=\"python main.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} --learning_rate ${{inputs.learning_rate}} --batch_size ${{inputs.batch_size}} --epochs ${{inputs.epochs}} --registered_model_name ${{inputs.registered_model_name}}\",\n",
        "    environment=\"aml-pytorch@latest\",\n",
        "    display_name=\"wine_quality_prediction\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submit the job \n",
        "\n",
        "It's now time to submit the job to run in Azure Machine Learning studio. This time you'll use `create_or_update`  on `ml_client`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1677262345449
        },
        "name": "create_job"
      },
      "outputs": [],
      "source": [
        "ml_client.create_or_update(job)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View job output and wait for job completion\n",
        "\n",
        "When you run the cell, the notebook output shows a link to the job's details page on Azure Studio. Alternatively, you can also select Jobs on the left navigation menu. A job is a grouping of many runs from a specified script or piece of code. Information for the run is stored under that job. The details page gives an overview of the job, the time it took to run, when it was created, etc. The page also has tabs to other information about the job such as metrics, Outputs + logs, and code. Listed below are the tabs available in the job's details page:\n",
        "\n",
        "\n",
        "#### IMPORTANT\n",
        "- There will be two jobs, first `prepare-image` which will be the job that spin's up your virtual Ubuntu machine. The second will have your experiment name, in this example `ml-demo`. The second can only run after the first is completed.\n",
        "- The job will take 2 to 3 minutes to run. It could take longer (up to 10 minutes) if the compute cluster has been scaled down to zero nodes and custom environment is still building. Once completed you can explore the outputs\n",
        "\n",
        "\n",
        "#### Jobs Output - main tabs\n",
        "\n",
        "* Overview: The overview section provides basic information about the job, including its status, start and end times, and the type of job that was run\n",
        "* Metrics: The metrics tab showcases key performance metrics from your model such as training score, f1 score, and precision score. \n",
        "* Images: This tab contains any images, such as confusion matrices, ROC curves, ect, that you have created during your run.\n",
        "* Outputs + logs: The Outputs + logs tab contains logs generated while the job was running. This tab assists in troubleshooting if anything goes wrong with your training script or model creation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stop compute instance\n",
        "\n",
        "If you're not going to use it now, stop the compute instance:\n",
        "\n",
        "1. In the studio, in the left navigation area, select **Compute**.\n",
        "1. In the top tabs, select **Compute instances**\n",
        "1. Select the compute instance in the list.\n",
        "1. On the top toolbar, select **Stop**.\n"
      ]
    }
  ],
  "metadata": {
    "categories": [
      "SDK v2",
      "tutorials"
    ],
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
